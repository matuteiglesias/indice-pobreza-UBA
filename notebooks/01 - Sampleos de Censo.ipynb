{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On this notebook we extract Censo 2010 individual data from their files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184550/3391910190.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./../../samplerCensoARG/'):\n",
    "    !git clone https://github.com/matuteiglesias/samplerCensoARG.git ./../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ./../../samplerCensoARG/samplear.py -dbp '/media/miglesia/Elements/suite/ext_CPV2010_basico_radio_pub' -f 0.01 -y 2005 2006 -n ARG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pd.read_csv('/home/miglesia/repositories/indice-pobreza-ExactasUBA/notebooks/../data/info/AGLO_rk/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar info accesoria (Regiones, aglomerados, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "\n",
    "radio_AGLO = pd.read_csv('https://raw.githubusercontent.com/matuteiglesias/Aglomerados-EPH-INDEC/main/result/radios_aglo_EPH.csv')\n",
    "radio_AGLO['radio'] = radio_AGLO.COD_2010.str.replace('XX', '99').astype(int)\n",
    "radio_AGLO['AGLOMERADO'] = radio_AGLO.eph_codagl\n",
    "radio_AGLO['NOMAGLO'] = radio_AGLO.eph_aglome\n",
    "\n",
    "radio_ref = radio_ref.drop(['AGLOMERADO'], axis = 1).merge(radio_AGLO[['radio','AGLOMERADO', 'NOMAGLO']], how = 'left')\n",
    "radio_ref['AGLOMERADO'] = radio_ref['AGLOMERADO'].fillna(0).astype(int)\n",
    "\n",
    "# radio_ref[['PROV','NOMPROV','DPTO', 'NOMDPTO']].drop_duplicates().to_csv('./../data/DPTO_PROV.csv', index = False)\n",
    "dpto_region = pd.read_csv('./../data/info/DPTO_PROV_Region.csv')\n",
    "radio_ref = radio_ref.merge(dpto_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANO4</th>\n",
       "      <th>Region</th>\n",
       "      <th>Reg_rk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>cuyo</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>gran_buenos_aires</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>patagonia</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>pampeana</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>noroeste</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ANO4             Region  Reg_rk\n",
       "0  2003               cuyo   0.500\n",
       "1  2003  gran_buenos_aires   0.833\n",
       "2  2003          patagonia   1.000\n",
       "3  2003           pampeana   0.667\n",
       "4  2003           noroeste   0.167"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "AGLO_rk = pd.read_csv('./../../encuestador-de-hogares/data/info/AGLO_rk')\n",
    "rk_table = AGLO_rk.set_index(['ANO4', 'AGLOMERADO']).unstack()\n",
    "AGLO_rk_filled = rk_table.fillna(rk_table.mean()).stack().reset_index()\n",
    "AGLO_rk = AGLO_rk_filled\n",
    "\n",
    "Reg_rk = pd.read_csv('./../../encuestador-de-hogares/data/info/Reg_rk')\n",
    "Reg_rk.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop sampleo de censo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CAMBIADOR DE CONVENCIONES NOMBRE REGION\n",
    "# # File path\n",
    "# # file_path = './../../encuestador-de-hogares/data/info/Reg_rk'\n",
    "# file_path = './../data/info/DPTO_PROV_Region.csv'\n",
    "\n",
    "\n",
    "# # Read the old copy\n",
    "# Reg_rk = pd.read_csv(file_path)\n",
    "\n",
    "# # Define the mapping dictionary\n",
    "regiones = {\n",
    "    'Gran Buenos Aires': 'gran_buenos_aires',\n",
    "    'Pampeana': 'pampeana',\n",
    "    'Noroeste': 'noroeste',\n",
    "    'Noreste': 'noreste',\n",
    "    'Patag√≥nica': 'patagonia',\n",
    "    'Cuyo': 'cuyo'\n",
    "}\n",
    "\n",
    "# # Update region names using the mapping dictionary\n",
    "# Reg_rk['Region'] = Reg_rk['Region'].map(regiones)\n",
    "\n",
    "# # Save the updated DataFrame back to the same file\n",
    "# Reg_rk.to_csv(file_path, index=False)\n",
    "\n",
    "# # Print a message to confirm the update\n",
    "# print(\"Region names have been updated and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n",
      "(2216657, 39)\n",
      "(2216657, 41)\n"
     ]
    }
   ],
   "source": [
    "frac = 0.05 ## Frac needs to be the fraction used in the sampling (eg. -f 0.01 needs frac = 0.01)\n",
    "startyr = 2019\n",
    "endyr = 2023\n",
    "\n",
    "for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "\n",
    "    ## EXTRACCION\n",
    "    ##############\n",
    "    print(yr)\n",
    "    input = pd.read_csv('./../../samplerCensoARG/data/censo_samples/table_f'+str(frac)+'_'+yr+'_ARG.csv')\n",
    "    table = input.copy()\n",
    "    table['ANO4'] = int(yr)\n",
    "    \n",
    "\n",
    "    ## TRANSFORMACION\n",
    "    ##################\n",
    "    # Adaptamos las categorias de respuestas para que iguales las de la EPH\n",
    "    ## VIVIENDA\n",
    "    table['V01'] = table['V01'].map({1:1, 2:6, 3:6, 4:2, 5:3, 6:4, 7:5, 8:6})\n",
    "    ## HOGAR\n",
    "    table['H06'] = table['H06'].map({1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:9})\n",
    "    table['H09'] = table['H09'].map({1:1, 2:2, 3:3, 4:4, 5:4, 6:4})\n",
    "    table['H16'] = table['H16'].clip(0, 9)\n",
    "    table['H14'] = table['H14'].map({1:1, 2:4, 3:2, 4:2, 5:4, 6:3, 7:4, 8:9})\n",
    "    table['H13'] = table['H13'].map({1:1, 2:2, 4:0})\n",
    "    table['IX_TOT'] = table['TOTPERS']\n",
    "    # PERSONA\n",
    "    table['P07'] = table['P07'].map({1:1, 2:2, 0:2})\n",
    "\n",
    "    ## Agregar Region\n",
    "    table = table.merge(dpto_region[['DPTO', 'Region']])\n",
    "\n",
    "    ## ID DE PERSONAS SIMULADAS:\n",
    "    # Semilla ids\n",
    "    file_size = os.path.getsize('./../../samplerCensoARG/data/censo_samples/table_f'+str(frac)+'_'+yr+'_ARG.csv')\n",
    "    np.random.seed(file_size)\n",
    "\n",
    "    ## Aplicar IDs\n",
    "    # Generate a unique random number for each row in the DataFrame\n",
    "    n_digits = 8; n_rows = len(table)\n",
    "    random_numbers = np.random.randint(10**(n_digits - 1), 10**n_digits, n_rows)\n",
    "    # Extract the last two digits of the year\n",
    "    last_two_digits_year = table['ANO4'].apply(lambda x: int(str(x)[-2:]))\n",
    "    \n",
    "    # unique ID\n",
    "    table.insert(0, 'ID', random_numbers * 100 + last_two_digits_year)\n",
    "\n",
    "    ## Agregar ranking de Region y Aglo\n",
    "    print(table.shape)\n",
    "    table = table.merge(AGLO_rk[['AGLOMERADO', 'ANO4', 'AGLO_rk']]).merge(Reg_rk[['Region', 'ANO4', 'Reg_rk']])\n",
    "    print(table.shape)\n",
    "    \n",
    "\n",
    "    # CARGA\n",
    "    #############\n",
    "    table.to_csv('/media/matias/Elements/suite/poblaciones/table_f'+str(frac)+'_'+yr+'_ARG.csv', index = False)  # Copias en carpeta yr_samples, en nuestra carpeta de indice de pobreza\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Source and Destination: Identify the source of your data (CSV files in this case) and the destination (e.g., a relational database).\n",
    "\n",
    "Create Tables and Relationships: Design the database schema to match the transformed data structure, including tables, columns, and relationships.\n",
    "\n",
    "Build ETL Pipeline: Develop a pipeline using ETL tools or scripting to extract, transform, and load the data as described above.\n",
    "\n",
    "Validation and Error Handling: Include validation steps to ensure data quality and error handling to deal with any issues that arise during the transformation.\n",
    "\n",
    "Scheduling and Monitoring: If the process needs to be run regularly, set up scheduling and monitoring to ensure that it runs smoothly and that any issues are quickly identified and addressed.\n",
    "\n",
    "Documentation and Compliance: Document the entire process, including the transformations and mappings, especially if there are regulatory compliance requirements related to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
