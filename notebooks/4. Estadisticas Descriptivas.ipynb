{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4. Estadísticas Descriptivas.ipynb**\n",
    "\n",
    "  - **4.2. Cálculo de estadísticas:** \n",
    "    - **4.2.1.** Estadísticas descriptivas para los datos monetarios ajustados.\n",
    "    - **4.2.2.** Procesamiento y guardado de diversos resultados estadísticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Módulos y Bibliotecas\n",
    "# Importar todos los módulos y bibliotecas necesarios.\n",
    "import pandas as pd\n",
    "import os\n",
    "from funciones import generate_Qs_from_year, sintetizar_datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuración de Directorios y Parámetros\n",
    "# Establecer y verificar las rutas de los directorios.\n",
    "# Definir los parámetros necesarios.\n",
    "if not os.path.exists('./../data/Pobreza/'):\n",
    "    os.makedirs('./../data/Pobreza/')\n",
    "\n",
    "# -------------------\n",
    "# Parameters and Configuration\n",
    "# -------------------\n",
    "\n",
    "FRAC = 0.02\n",
    "START_YEAR = 2023\n",
    "END_YEAR = 2024\n",
    "EXPERIMENT_TAG = 'ARG'\n",
    "\n",
    "PATH_POBREZA = './../data/Pobreza/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Exploración y Preprocesamiento de Datos\n",
    "# # Cargar conjuntos de datos.\n",
    "# # Explorar datos iniciales.\n",
    "# # Preparar datos para síntesis.\n",
    "\n",
    "# # Carga de Datos Auxiliares: Datos del adulto equivalente y Canasta básica regional deflactada\n",
    "# ad_eq = pd.read_csv('./../data/info/adulto_eq.csv')\n",
    "# url_canasta_q_deflac = 'https://raw.githubusercontent.com/matuteiglesias/canastasINDEC/main/data/CB_Reg_defl_Q.csv'\n",
    "# CB_ipc = pd.read_csv(url_canasta_q_deflac)\n",
    "# print(CB_ipc.tail())\n",
    "\n",
    "# # Preparación de la Información Geográfica\n",
    "# radio_ref = pd.read_csv('./../data/info/radio_ref.csv', usecols = ['RADIO_REF_ID', 'DPTO', 'FRAC_REF_ID', 'NOMDPTO', 'radio'])\n",
    "# dpto_region = pd.read_csv('./../data/info/DPTO_PROV_Region.csv')\n",
    "# radio_ref = radio_ref.merge(dpto_region)\n",
    "# radio_ref['COD_2010'] = radio_ref['radio'].astype(str).str.zfill(9)\n",
    "# radio_ref = radio_ref.drop('radio', axis = 1)\n",
    "# radio_ref_cols = radio_ref\n",
    "\n",
    "# claves_dptos = pd.read_csv('https://raw.githubusercontent.com/matuteiglesias/elecciones-ARG/main/datos/BD/claves_dptos_ref.csv')\n",
    "# claves_dptos_cols = claves_dptos[['distrito_id', 'seccion_id', 'IN1', 'NAM']].drop_duplicates()\n",
    "# display('dtypes claves_dptos_ref', claves_dptos_cols.dtypes)\n",
    "\n",
    "# radios_circuitos_secciones_ref = pd.read_csv('./../../CNE-INDEC-georef/info/radios_circuitos_secciones_ref.csv')\n",
    "# radios_circuitos_secciones_ref = radios_circuitos_secciones_ref[['COD_2010', 'distrito_id', 'seccion_id', 'seccion_nombre', 'circuito']]\n",
    "# display('radios_circuitos_secciones_ref', radios_circuitos_secciones_ref.max())\n",
    "\n",
    "# DPTO_Region = radio_ref[['DPTO', 'Region']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Estadisticas Descriptivas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Nombres de los archivos\n",
    "# personas_ingresos_Q_file = f'./../data/Pobreza/personas_ingresos_f{FRAC}_{Q}_{EXPERIMENT_TAG}.csv'\n",
    "# pobreza_hogares_file = f'./../data/Pobreza/pobreza_hogares_f{FRAC}_q{Q}.csv'\n",
    "# hogares_geo_file = f'./../data/Pobreza/hogares_geo_f{FRAC}_{Q.split(\"-\")[0]}_{EXPERIMENT_TAG}.csv'\n",
    "\n",
    "# # Cargar los archivos (solo las primeras 5 filas)\n",
    "# personas_ingresos_Q = pd.read_csv(personas_ingresos_Q_file, nrows=5)\n",
    "# pobreza_hogares = pd.read_csv(pobreza_hogares_file, nrows=5)\n",
    "# hogares_geo = pd.read_csv(hogares_geo_file, nrows=5)\n",
    "\n",
    "# # Merges\n",
    "# info_personas = personas_ingresos_Q.merge(pobreza_hogares, on=['HOGAR_REF_ID', 'Q'], how='left').merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "# info_hogares = pobreza_hogares.merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "\n",
    "# # Mostrar columnas\n",
    "# print(\"Columnas de info_personas:\", info_personas.columns)\n",
    "# print(\"\\nColumnas de info_hogares:\", info_hogares.columns)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## **4. Preparación de Datos para Síntesis**\n",
    "\n",
    "# %%\n",
    "# # Lista de agrupaciones para las transformaciones\n",
    "# groupersP = [['Q', 'Total']]\n",
    "# groupersH = [['Q', 'Total']]\n",
    "\n",
    "groupersP = [ # ['Q', 'distrito_id',\t'seccion_id', 'seccion_nombre', 'circuito'], \n",
    "    ['Q','Total'], ['Q','AGLOSI'], ['Q','AGLOMERADO'], ['Q','Region'], ['Q','PROV'], ['Q','DPTO'], ['Q','P0910'], \n",
    "             ['Q','Region', 'AGLOSI'], ['Q', 'PROV', 'AGLOSI']]\n",
    "\n",
    "groupersH = [['Q','Total'], ['Q','AGLOSI'], ['Q','AGLOMERADO'], ['Q','Region'], ['Q','PROV'], ['Q','DPTO'],\n",
    "            #  ]#,\n",
    "             ['Q','Region', 'AGLOSI'], ['Q', 'PROV', 'AGLOSI']]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha de hoy: 2023-10\n",
      "Índice de precios al consumidor: 32.44 (base 2016-01 = 1)\n"
     ]
    }
   ],
   "source": [
    "## A pesos actuales\n",
    "from datetime import datetime\n",
    "\n",
    "# Cargar el índice de precios al consumidor (CPI) desde la fuente\n",
    "cpi = pd.read_csv('https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_M.csv', index_col=0)\n",
    "cpi.index = pd.to_datetime(cpi.index)\n",
    "\n",
    "# Obtener la fecha de hoy en formato año-mes\n",
    "hoy = datetime.today().strftime('%Y-%m')\n",
    "\n",
    "# Calcular el ratio de precios de hoy con respecto a los precios con índice en base al modelo\n",
    "ix = cpi.loc[hoy, 'index'].values[0] / cpi.loc['2016-01', 'index'].values[0]\n",
    "\n",
    "# Lista de columnas relacionadas con montos en pesos\n",
    "columnas_pesos = ['P47T_persona', 'P47T_hogar', 'CBA', 'gap_indigencia', 'CBT', 'gap_pobreza']\n",
    "\n",
    "## Print log of the current day and index compared to base, format 2 decimals, unit is base 01-01-2016\n",
    "print(f'Fecha de hoy: {hoy}')\n",
    "print(f'Índice de precios al consumidor: {ix:.2f} (base 2016-01 = 1)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for year 2023...\n",
      "Loading data for Q: 2023-02-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data for Q: 2023-02-15\n",
      "Lin to log for  P47T_persona . Current mean:  2.297432536414374\n",
      "Check for  CBA . Mean:  8751.581426975912\n",
      "Check for  CBA . Median:  4916.61\n",
      "Check for  CBT . Mean:  19364.581416034547\n",
      "Check for  CBT . Median:  10866.95\n",
      "Check for  CBA . Mean:  3984.2839264418876\n",
      "Check for  CBA . Median:  3654.17\n",
      "Check for  CBT . Mean:  8828.319110696584\n",
      "Check for  CBT . Median:  8076.34\n",
      "Transforming data for Q: 2023-02-15\n",
      "./../data/results//stats_P_Q-Total_sample0.02.csv\n",
      "./../data/results//stats_P_Q-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_P_Q-AGLOMERADO_sample0.02.csv\n",
      "./../data/results//stats_P_Q-Region_sample0.02.csv\n",
      "./../data/results//stats_P_Q-PROV_sample0.02.csv\n",
      "./../data/results//stats_P_Q-DPTO_sample0.02.csv\n",
      "./../data/results//stats_P_Q-P0910_sample0.02.csv\n",
      "./../data/results//stats_P_Q-Region-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_P_Q-PROV-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-Total_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-AGLOMERADO_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-Region_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-PROV_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-DPTO_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-P0910_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-Region-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_PAGLO_Q-PROV-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-Total_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-AGLOMERADO_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-Region_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-PROV_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-DPTO_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-P0910_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-Region-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_M24_Q-PROV-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_H_Q-Total_sample0.02.csv\n",
      "./../data/results//stats_H_Q-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_H_Q-AGLOMERADO_sample0.02.csv\n",
      "./../data/results//stats_H_Q-Region_sample0.02.csv\n",
      "./../data/results//stats_H_Q-PROV_sample0.02.csv\n",
      "./../data/results//stats_H_Q-DPTO_sample0.02.csv\n",
      "./../data/results//stats_H_Q-Region-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_H_Q-PROV-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_Hp_Q-Total_sample0.02.csv\n",
      "./../data/results//stats_Hp_Q-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_Hp_Q-AGLOMERADO_sample0.02.csv\n",
      "./../data/results//stats_Hp_Q-Region_sample0.02.csv\n",
      "./../data/results//stats_Hp_Q-PROV_sample0.02.csv\n",
      "./../data/results//stats_Hp_Q-DPTO_sample0.02.csv\n",
      "./../data/results//stats_Hp_Q-Region-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_Hp_Q-PROV-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_Hi_Q-Total_sample0.02.csv\n",
      "./../data/results//stats_Hi_Q-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_Hi_Q-AGLOMERADO_sample0.02.csv\n",
      "./../data/results//stats_Hi_Q-Region_sample0.02.csv\n",
      "./../data/results//stats_Hi_Q-PROV_sample0.02.csv\n",
      "./../data/results//stats_Hi_Q-DPTO_sample0.02.csv\n",
      "./../data/results//stats_Hi_Q-Region-AGLOSI_sample0.02.csv\n",
      "./../data/results//stats_Hi_Q-PROV-AGLOSI_sample0.02.csv\n",
      "Loading data for Q: 2023-05-15\n",
      "Warning: ./../data/Pobreza/individual_income_sample0.02_2023-05-15_ARG.csv does not exist. Skipping Q: 2023-05-15\n",
      "Loading data for Q: 2023-08-15\n",
      "Warning: ./../data/Pobreza/individual_income_sample0.02_2023-08-15_ARG.csv does not exist. Skipping Q: 2023-08-15\n",
      "Loading data for Q: 2023-11-15\n",
      "Warning: ./../data/Pobreza/individual_income_sample0.02_2023-11-15_ARG.csv does not exist. Skipping Q: 2023-11-15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "from numpy import power\n",
    "import os\n",
    "results_path = './../data/results/'\n",
    "\n",
    "# Create dictionaries for each base universe\n",
    "all_data = {\n",
    "    'P': {},\n",
    "    'PAGLO': {},\n",
    "    'M24': {},\n",
    "    'H': {},\n",
    "    'Hp': {},\n",
    "    'Hi': {}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Loop through the years\n",
    "for yr in range(START_YEAR, END_YEAR):\n",
    "# for yr in range(2015, 2016):\n",
    "    yr = str(yr)  # Convert year to string for consistency with quarters\n",
    "    print(f\"Processing data for year {yr}...\")\n",
    "\n",
    "    # Procesamiento de datos para cada trimestre del año en cuestión.\n",
    "    relevant_quarters = generate_Qs_from_year(yr)\n",
    "    for Q in relevant_quarters:\n",
    "        # LOADING AND MERGING    \n",
    "        print(f\"Loading data for Q: {Q}\")\n",
    "        # Nombres de los archivos dependientes de Q\n",
    "        personas_ingresos_Q_file = f'{PATH_POBREZA}individual_income_sample{FRAC}_{Q}_{EXPERIMENT_TAG}.csv'\n",
    "        pobreza_hogares_file = f'{PATH_POBREZA}household_poverty_sample{FRAC}_q{Q}.csv'\n",
    "        hogares_geo_file = f'{PATH_POBREZA}geo_households_sample{FRAC}_{Q.split(\"-\")[0]}_{EXPERIMENT_TAG}.csv'\n",
    "\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(personas_ingresos_Q_file):\n",
    "            print(f\"Warning: {personas_ingresos_Q_file} does not exist. Skipping Q: {Q}\")\n",
    "            continue\n",
    "        \n",
    "        # Cargar los archivos\n",
    "        personas_ingresos_Q = pd.read_csv(personas_ingresos_Q_file)\n",
    "        pobreza_hogares = pd.read_csv(pobreza_hogares_file)\n",
    "        hogares_geo = pd.read_csv(hogares_geo_file)\n",
    "\n",
    "        # Fusiones para obtener los datasets consolidados\n",
    "        print(f\"Merging data for Q: {Q}\")\n",
    "        info_personas = personas_ingresos_Q.merge(pobreza_hogares, on=['HOGAR_REF_ID', 'Q'], how='left'\n",
    "                                                ).merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "        info_hogares = pobreza_hogares.merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "\n",
    "        # Agregar AGLO SI, y Total Pais.\n",
    "        info_personas['AGLOSI'] = info_personas.AGLOMERADO != 0\n",
    "        info_personas['Total'] = True\n",
    "        info_hogares['AGLOSI'] = info_hogares.AGLOMERADO != 0\n",
    "        info_hogares['Total'] = True\n",
    "\n",
    "\n",
    "        ## Adaptar datos (Pesos actuales, AGLOS si, IDFRAC)\n",
    "        for df in [info_personas, info_hogares]:\n",
    "            for col in columnas_pesos:\n",
    "                if col in df.columns: \n",
    "                    if col == 'P47T_persona': \n",
    "                        print('Lin to log for ', col, '. Current mean: ', df[col].mean())\n",
    "                        df[col] = power(10, df[col]) - 1   ## Log -> Lin\n",
    "                    if col in ['CBA', 'CBT', 'CB_EQUIV']: \n",
    "                        print('Check for ', col, '. Mean: ', df[col].mean())\n",
    "                        print('Check for ', col, '. Median: ', df[col].median())\n",
    "                    \n",
    "                    df[col] = (ix*df[col]).round(-1).astype(int)  ## Pesos actuales\n",
    "\n",
    "\n",
    "        # Define mapping of base_str to its dataset and associated grouper\n",
    "        mappings = {\n",
    "            'P': (info_personas, groupersP),\n",
    "            'PAGLO': (info_personas[info_personas.AGLOSI], groupersP),\n",
    "            'M24': (info_personas[info_personas.P03 >= 24], groupersP),\n",
    "            # 'M18': (info_personas[info_personas.P03 >= 18], groupersP),\n",
    "            'H': (info_hogares, groupersH),\n",
    "            'Hp': (info_hogares[info_hogares.Pobreza], groupersH),\n",
    "            'Hi': (info_hogares[info_hogares.Indigencia], groupersH)\n",
    "        }\n",
    "\n",
    "\n",
    "        # TRANSFORMACIONES\n",
    "        print(f\"Transforming data for Q: {Q}\")\n",
    "        for base_str, (data_subset, grouper_list) in mappings.items():\n",
    "            # Create a separate dictionary for the current base string\n",
    "\n",
    "            for grouper in grouper_list:\n",
    "                # Synthesize data\n",
    "                new_data = sintetizar_datos(data_subset, grouper, base_str, FRAC)\n",
    "                # timestamp = dt.datetime.now().isoformat()\n",
    "                \n",
    "                # Construct filename\n",
    "                filename = f'{results_path}/stats_{base_str}_{\"-\".join(grouper)}_sample{FRAC}.csv'\n",
    "                \n",
    "                # If file exists, read and append data\n",
    "                if os.path.exists(filename):\n",
    "                    existing_data = pd.read_csv(filename)\n",
    "                    combined_data = pd.concat([existing_data, new_data], axis=0)\n",
    "                    df = combined_data\n",
    "                else:\n",
    "                    # If file does not exist, simply save the new data\n",
    "                    df = new_data\n",
    "                    \n",
    "\n",
    "                # Group by all columns except 'valor' and 'timestamp', and get the index of the latest timestamp\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                idx = df.groupby(df.columns.difference(['valor', 'timestamp']).tolist())['timestamp'].idxmax()\n",
    "        \n",
    "                # Filter the dataframe using these indices\n",
    "                result = df.loc[idx].drop_duplicates()\n",
    "                print(filename)\n",
    "\n",
    "                result.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "# %%\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gap_pobreza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gap_pobreza'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/repos/indice-pobreza-UBA/notebooks/4. Estadisticas Descriptivas.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matias/repos/indice-pobreza-UBA/notebooks/4.%20Estadisticas%20Descriptivas.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[col]\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gap_pobreza'"
     ]
    }
   ],
   "source": [
    "df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col] = (ix*df[col]).round(-1).astype(int)  ## Pesos actuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/repos/indice-pobreza-UBA/notebooks/4. Estadisticas Descriptivas.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matias/repos/indice-pobreza-UBA/notebooks/4.%20Estadisticas%20Descriptivas.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m xx\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xx' is not defined"
     ]
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for Q: 2016-05-15\n",
      "Merging data for Q: 2016-05-15\n",
      "Lin to log for  P47T_persona . Current mean:  2.2451759744795696\n",
      "Check for  CBA . Mean:  13841.478152344998\n",
      "Check for  CBA . Median:  4274.693642028486\n",
      "Check for  CBT . Mean:  33725.20457587036\n",
      "Check for  CBT . Median:  10393.94415371896\n",
      "Check for  CBA . Mean:  3464.866082694004\n",
      "Check for  CBA . Median:  3168.9369270558173\n",
      "Check for  CBT . Mean:  8444.746040978942\n",
      "Check for  CBT . Median:  7721.42690484621\n"
     ]
    }
   ],
   "source": [
    "# # LOADING AND MERGING    \n",
    "# Q = '2016-05-15'\n",
    "# print(f\"Loading data for Q: {Q}\")\n",
    "# # Nombres de los archivos dependientes de Q\n",
    "# personas_ingresos_Q_file = f'{PATH_POBREZA}personas_ingresos_f{FRAC}_{Q}_{EXPERIMENT_TAG}.csv'\n",
    "# pobreza_hogares_file = f'{PATH_POBREZA}pobreza_hogares_f{FRAC}_q{Q}.csv'\n",
    "# hogares_geo_file = f'{PATH_POBREZA}hogares_geo_f{FRAC}_{Q.split(\"-\")[0]}_{EXPERIMENT_TAG}.csv'\n",
    "\n",
    "# # # Check if the file exists\n",
    "# # if not os.path.exists(personas_ingresos_Q_file):\n",
    "# #     print(f\"Warning: {personas_ingresos_Q_file} does not exist. Skipping Q: {Q}\")\n",
    "# #     continue\n",
    "\n",
    "# # Cargar los archivos\n",
    "# personas_ingresos_Q = pd.read_csv(personas_ingresos_Q_file)\n",
    "# pobreza_hogares = pd.read_csv(pobreza_hogares_file)\n",
    "# hogares_geo = pd.read_csv(hogares_geo_file)\n",
    "\n",
    "# # Fusiones para obtener los datasets consolidados\n",
    "# print(f\"Merging data for Q: {Q}\")\n",
    "# info_personas = personas_ingresos_Q.merge(pobreza_hogares, on=['HOGAR_REF_ID', 'Q'], how='left'\n",
    "#                                         ).merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "# info_hogares = pobreza_hogares.merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "\n",
    "# # Agregar AGLO SI, y Total Pais.\n",
    "# info_personas['AGLOSI'] = info_personas.AGLOMERADO != 0\n",
    "# info_personas['Total'] = True\n",
    "# info_hogares['AGLOSI'] = info_hogares.AGLOMERADO != 0\n",
    "# info_hogares['Total'] = True\n",
    "\n",
    "\n",
    "# ## Adaptar datos (Pesos actuales, AGLOS si, IDFRAC)\n",
    "# for df in [info_personas, info_hogares]:\n",
    "#     for col in columnas_pesos:\n",
    "#         if col in df.columns: \n",
    "#             if col == 'P47T_persona': \n",
    "#                 print('Lin to log for ', col, '. Current mean: ', df[col].mean())\n",
    "#                 df[col] = power(10, df[col]) - 1   ## Log -> Lin\n",
    "#             if col in ['CBA', 'CBT', 'CB_EQUIV']: \n",
    "#                 print('Check for ', col, '. Mean: ', df[col].mean())\n",
    "#                 print('Check for ', col, '. Median: ', df[col].median())\n",
    "            \n",
    "#             df[col] = (ix*df[col]).round(-1).astype(int)  ## Pesos actuales\n",
    "\n",
    "\n",
    "# # Define mapping of base_str to its dataset and associated grouper\n",
    "# mappings = {\n",
    "#     # 'P': (info_personas, groupersP),\n",
    "#     # 'PAGLO': (info_personas[info_personas.AGLOSI], groupersP),\n",
    "#     'M24': (info_personas[info_personas.P03 >= 24], groupersP),\n",
    "#     # 'M18': (info_personas[info_personas.P03 >= 18], groupersP),\n",
    "#     # 'H': (info_hogares, groupersH),\n",
    "#     # 'Hp': (info_hogares[info_hogares.Pobreza], groupersH),\n",
    "#     # 'Hi': (info_hogares[info_hogares.Indigencia], groupersH)\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data for Q: 2016-05-15\n",
      "./../data/results//result_M24_Q-Total_0.01.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # TRANSFORMACIONES\n",
    "# print(f\"Transforming data for Q: {Q}\")\n",
    "# for base_str, (data_subset, grouper_list) in mappings.items():\n",
    "#     # Create a separate dictionary for the current base string\n",
    "\n",
    "#     for grouper in grouper_list:\n",
    "#         # Synthesize data\n",
    "#         new_data = sintetizar_datos(data_subset, grouper, base_str, FRAC)\n",
    "#         # timestamp = dt.datetime.now().isoformat()\n",
    "        \n",
    "#         # Construct filename\n",
    "#         filename = f'{results_path}/result_{base_str}_{\"-\".join(grouper)}_{FRAC}.csv'\n",
    "        \n",
    "#         # If file exists, read and append data\n",
    "#         if os.path.exists(filename):\n",
    "#             existing_data = pd.read_csv(filename)\n",
    "#             combined_data = pd.concat([existing_data, new_data], axis=0)\n",
    "#             df = combined_data\n",
    "#         else:\n",
    "#             # If file does not exist, simply save the new data\n",
    "#             df = new_data\n",
    "            \n",
    "\n",
    "#         # Group by all columns except 'valor' and 'timestamp', and get the index of the latest timestamp\n",
    "#         df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "#         idx = df.groupby(df.columns.difference(['valor', 'timestamp']).tolist())['timestamp'].idxmax()\n",
    "\n",
    "#         # Filter the dataframe using these indices\n",
    "#         result = df.loc[idx].drop_duplicates()\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../data/results//result_M24_Q-Total_0.01.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observable</th>\n",
       "      <th>sintetico</th>\n",
       "      <th>base</th>\n",
       "      <th>Q</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Total</th>\n",
       "      <th>valor</th>\n",
       "      <th>frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>CBA</td>\n",
       "      <td>median</td>\n",
       "      <td>M24</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>2023-10-14 22:17:40.373228</td>\n",
       "      <td>True</td>\n",
       "      <td>114090.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>CBA</td>\n",
       "      <td>median</td>\n",
       "      <td>M24</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>2023-10-14 22:16:01.588361</td>\n",
       "      <td>True</td>\n",
       "      <td>115450.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>CBA</td>\n",
       "      <td>median</td>\n",
       "      <td>M24</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>2023-10-14 22:13:08.995275</td>\n",
       "      <td>True</td>\n",
       "      <td>116230.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>CBA</td>\n",
       "      <td>median</td>\n",
       "      <td>M24</td>\n",
       "      <td>2017-11-15</td>\n",
       "      <td>2023-10-14 22:14:32.979933</td>\n",
       "      <td>True</td>\n",
       "      <td>116310.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>CBA</td>\n",
       "      <td>median</td>\n",
       "      <td>M24</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>2023-10-14 22:11:54.145558</td>\n",
       "      <td>True</td>\n",
       "      <td>116690.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    observable sintetico base           Q                  timestamp  Total  \\\n",
       "295        CBA    median  M24  2018-05-15 2023-10-14 22:17:40.373228   True   \n",
       "267        CBA    median  M24  2018-02-15 2023-10-14 22:16:01.588361   True   \n",
       "211        CBA    median  M24  2017-08-15 2023-10-14 22:13:08.995275   True   \n",
       "239        CBA    median  M24  2017-11-15 2023-10-14 22:14:32.979933   True   \n",
       "183        CBA    median  M24  2017-05-15 2023-10-14 22:11:54.145558   True   \n",
       "\n",
       "        valor  frac  \n",
       "295 114090.00  0.01  \n",
       "267 115450.00  0.01  \n",
       "211 116230.00  0.01  \n",
       "239 116310.00  0.01  \n",
       "183 116690.00  0.01  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result.loc[result.observable == 'CBA'].sort_values('valor').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observable</th>\n",
       "      <th>sintetico</th>\n",
       "      <th>base</th>\n",
       "      <th>Q</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Total</th>\n",
       "      <th>valor</th>\n",
       "      <th>frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2022-05-15</td>\n",
       "      <td>2023-10-14 23:08:23.035897</td>\n",
       "      <td>True</td>\n",
       "      <td>273490.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>2023-10-14 23:16:42.557935</td>\n",
       "      <td>True</td>\n",
       "      <td>279670.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>2023-10-14 22:09:46.041165</td>\n",
       "      <td>True</td>\n",
       "      <td>583460.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2016-08-15</td>\n",
       "      <td>2023-10-14 22:08:52.855965</td>\n",
       "      <td>True</td>\n",
       "      <td>587820.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2016-05-15</td>\n",
       "      <td>2023-10-27 00:00:38.994781</td>\n",
       "      <td>True</td>\n",
       "      <td>592300.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    observable sintetico base           Q                  timestamp  Total  \\\n",
       "742        CBA      mean  M24  2022-05-15 2023-10-14 23:08:23.035897   True   \n",
       "798        CBA      mean  M24  2022-11-15 2023-10-14 23:16:42.557935   True   \n",
       "126        CBA      mean  M24  2016-11-15 2023-10-14 22:09:46.041165   True   \n",
       "98         CBA      mean  M24  2016-08-15 2023-10-14 22:08:52.855965   True   \n",
       "20         CBA      mean  M24  2016-05-15 2023-10-27 00:00:38.994781   True   \n",
       "\n",
       "        valor  frac  \n",
       "742 273490.00  0.01  \n",
       "798 279670.00  0.01  \n",
       "126 583460.00  0.01  \n",
       "98  587820.00  0.01  \n",
       "20  592300.00  0.01  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result.loc[result.observable == 'CBA'].sort_values('valor').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observable</th>\n",
       "      <th>sintetico</th>\n",
       "      <th>base</th>\n",
       "      <th>Q</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Total</th>\n",
       "      <th>valor</th>\n",
       "      <th>frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2022-05-15</td>\n",
       "      <td>2023-10-14 23:08:23.035897</td>\n",
       "      <td>True</td>\n",
       "      <td>273490.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>2023-10-14 23:16:42.557935</td>\n",
       "      <td>True</td>\n",
       "      <td>279670.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2016-05-15</td>\n",
       "      <td>2023-10-14 22:08:06.963723</td>\n",
       "      <td>True</td>\n",
       "      <td>577740.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2016-08-15</td>\n",
       "      <td>2023-10-14 22:08:52.855965</td>\n",
       "      <td>True</td>\n",
       "      <td>587820.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CBA</td>\n",
       "      <td>mean</td>\n",
       "      <td>M24</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>2023-10-26 23:46:31.548713</td>\n",
       "      <td>True</td>\n",
       "      <td>598170.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    observable sintetico base           Q                  timestamp  Total  \\\n",
       "742        CBA      mean  M24  2022-05-15 2023-10-14 23:08:23.035897   True   \n",
       "798        CBA      mean  M24  2022-11-15 2023-10-14 23:16:42.557935   True   \n",
       "70         CBA      mean  M24  2016-05-15 2023-10-14 22:08:06.963723   True   \n",
       "98         CBA      mean  M24  2016-08-15 2023-10-14 22:08:52.855965   True   \n",
       "20         CBA      mean  M24  2016-11-15 2023-10-26 23:46:31.548713   True   \n",
       "\n",
       "        valor  frac  \n",
       "742 273490.00  0.01  \n",
       "798 279670.00  0.01  \n",
       "70  577740.00  0.01  \n",
       "98  587820.00  0.01  \n",
       "20  598170.00  0.01  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result.loc[result.observable == 'CBA'].sort_values('valor').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for year 2015...\n",
      "Loading data for Q: 2015-02-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data for Q: 2015-02-15\n",
      "Lin to log for  P47T_persona . Current mean:  2.2928558729927033\n",
      "Transforming data for Q: 2015-02-15\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "from numpy import power\n",
    "import os\n",
    "results_path = './../data/results/'\n",
    "\n",
    "# Create dictionaries for each base universe\n",
    "all_data = {\n",
    "    'P': {},\n",
    "    'PAGLO': {},\n",
    "    'M24': {},\n",
    "    'H': {},\n",
    "    'Hp': {},\n",
    "    'Hi': {}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Loop through the years\n",
    "for yr in range(START_YEAR, END_YEAR):\n",
    "# for yr in range(2015, 2016):\n",
    "    yr = str(yr)  # Convert year to string for consistency with quarters\n",
    "    print(f\"Processing data for year {yr}...\")\n",
    "\n",
    "    # Procesamiento de datos para cada trimestre del año en cuestión.\n",
    "    relevant_quarters = generate_Qs_from_year(yr)\n",
    "    for Q in relevant_quarters:\n",
    "        # LOADING AND MERGING    \n",
    "        print(f\"Loading data for Q: {Q}\")\n",
    "        # Nombres de los archivos dependientes de Q\n",
    "        personas_ingresos_Q_file = f'{PATH_POBREZA}personas_ingresos_f{FRAC}_{Q}_{EXPERIMENT_TAG}.csv'\n",
    "        pobreza_hogares_file = f'{PATH_POBREZA}pobreza_hogares_f{FRAC}_q{Q}.csv'\n",
    "        hogares_geo_file = f'{PATH_POBREZA}hogares_geo_f{FRAC}_{Q.split(\"-\")[0]}_{EXPERIMENT_TAG}.csv'\n",
    "\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(personas_ingresos_Q_file):\n",
    "            print(f\"Warning: {personas_ingresos_Q_file} does not exist. Skipping Q: {Q}\")\n",
    "            continue\n",
    "        \n",
    "        # Cargar los archivos\n",
    "        personas_ingresos_Q = pd.read_csv(personas_ingresos_Q_file)\n",
    "        pobreza_hogares = pd.read_csv(pobreza_hogares_file)\n",
    "        hogares_geo = pd.read_csv(hogares_geo_file)\n",
    "\n",
    "        # Fusiones para obtener los datasets consolidados\n",
    "        print(f\"Merging data for Q: {Q}\")\n",
    "        info_personas = personas_ingresos_Q.merge(pobreza_hogares, on=['HOGAR_REF_ID', 'Q'], how='left'\n",
    "                                                ).merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "        info_hogares = pobreza_hogares.merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "\n",
    "        # Agregar AGLO SI, y Total Pais.\n",
    "        info_personas['AGLOSI'] = info_personas.AGLOMERADO != 0\n",
    "        info_personas['Total'] = True\n",
    "        info_hogares['AGLOSI'] = info_hogares.AGLOMERADO != 0\n",
    "        info_hogares['Total'] = True\n",
    "\n",
    "\n",
    "        ## Adaptar datos (Pesos actuales, AGLOS si, IDFRAC)\n",
    "        for df in [info_personas, info_hogares]:\n",
    "            for col in columnas_pesos:\n",
    "                if col in df.columns: \n",
    "                    if col == 'P47T_persona': \n",
    "                        print('Lin to log for ', col, '. Current mean: ', df[col].mean())\n",
    "                        df[col] = power(10, df[col]) - 1   ## Log -> Lin\n",
    "                    df[col] = (ix*df[col]).round(-1).astype(int)  ## Pesos actuales\n",
    "\n",
    "\n",
    "        # Define mapping of base_str to its dataset and associated grouper\n",
    "        mappings = {\n",
    "            'P': (info_personas, groupersP),\n",
    "            # 'PAGLO': (info_personas[info_personas.AGLOSI], groupersP),\n",
    "            # 'M24': (info_personas[info_personas.P03 >= 24], groupersP),\n",
    "            'M18': (info_personas[info_personas.P03 >= 18], groupersP),\n",
    "            'H': (info_hogares, groupersH),\n",
    "            'Hp': (info_hogares[info_hogares.Pobreza], groupersH),\n",
    "            'Hi': (info_hogares[info_hogares.Indigencia], groupersH)\n",
    "        }\n",
    "\n",
    "\n",
    "        # TRANSFORMACIONES\n",
    "        print(f\"Transforming data for Q: {Q}\")\n",
    "        for base_str, (data_subset, grouper_list) in mappings.items():\n",
    "            # Create a separate dictionary for the current base string\n",
    "\n",
    "            for grouper in grouper_list:\n",
    "                # Synthesize data\n",
    "                new_data = sintetizar_datos(data_subset, grouper, base_str, FRAC)\n",
    "                # timestamp = dt.datetime.now().isoformat()\n",
    "                \n",
    "                # Construct filename\n",
    "                filename = f'{results_path}/result_{base_str}_{\"-\".join(grouper)}_{FRAC}.csv'\n",
    "                \n",
    "                # If file exists, read and append data\n",
    "                if os.path.exists(filename):\n",
    "                    existing_data = pd.read_csv(filename)\n",
    "                    combined_data = pd.concat([existing_data, new_data], axis=0)\n",
    "                    df = combined_data\n",
    "                else:\n",
    "                    # If file does not exist, simply save the new data\n",
    "                    df = new_data\n",
    "                    \n",
    "\n",
    "                # Group by all columns except 'valor' and 'timestamp', and get the index of the latest timestamp\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                idx = df.groupby(df.columns.difference(['valor', 'timestamp']).tolist())['timestamp'].idxmax()\n",
    "        \n",
    "                # Filter the dataframe using these indices\n",
    "                result = df.loc[idx].drop_duplicates()\n",
    "\n",
    "                result.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "# %%\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
