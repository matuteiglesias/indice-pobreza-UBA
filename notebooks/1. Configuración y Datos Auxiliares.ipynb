{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **1. Configuración y Datos Auxiliares.ipynb**\n",
    "  - **1.1. Configuración del entorno:** Importación de bibliotecas necesarias y configuración.\n",
    "  - **1.2. Repositorios:**\n",
    "    - **1.2.1.** Verificación de la existencia de repositorios.\n",
    "    - **1.2.2.** Clonación de repositorios si es necesario.\n",
    "  - **1.3. Carga de datos auxiliares:** \n",
    "    - **1.3.1.** Importación de datos de encuestas.\n",
    "    - **1.3.2.** Importación y procesamiento del índice de precios al consumidor (IPC).\n",
    "    - **1.3.3.** Importación de identificadores geográficos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Libraries and Imports\n",
    "# -------------------\n",
    "import subprocess\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from funciones import log_message, transform_censo_data, generate_unique_ids, generate_Qs\n",
    "\n",
    "# -------------------\n",
    "# Parameters and Configuration\n",
    "# -------------------\n",
    "\n",
    "FRAC = 0.02\n",
    "START_YEAR = 2003\n",
    "END_YEAR = 2024\n",
    "EXPERIMENT_TAG = 'ARG'\n",
    "\n",
    "# REPO_PATH = './../../samplerCensoARG/'\n",
    "REPO_PATH = './../../samplerCensoARG/'\n",
    "DATABASE_PATH = \"/media/matias/Elements/suite/ext_CPV2010_basico_radio_pub\"\n",
    "# -------------------\n",
    "# Repository Operations\n",
    "# -------------------\n",
    "\n",
    "# Check and clone the repository if necessary\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    !git clone https://github.com/matuteiglesias/samplerCensoARG.git REPO_PATH\n",
    "\n",
    "# -------------------\n",
    "# File Operations\n",
    "# -------------------\n",
    "\n",
    "# path = './../../encuestador-de-hogares/fitted_RF/clf4_'\n",
    "# allFiles = sorted(glob.glob(path +'*'))\n",
    "# allqs = [f[-14:-4] for f in allFiles]\n",
    "# print(allqs)\n",
    "\n",
    "# # Ensure the results directory exists\n",
    "# if not os.path.exists('./../data/resultados'):\n",
    "#     os.makedirs('./../data/resultados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Check and clone repository if necessary\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Load radio reference\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cargar referencia de radios y otros datos auxiliares\n",
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Load agglomerate information and merge with radio reference\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargamos información sobre los aglomerados\n",
    "radio_AGLO = pd.read_csv('https://raw.githubusercontent.com/matuteiglesias/Aglomerados-EPH-INDEC/main/result/radios_aglo_EPH.csv')\n",
    "radio_AGLO['radio'] = radio_AGLO.COD_2010.str.replace('XX', '99').astype(int)\n",
    "radio_AGLO['AGLOMERADO'] = radio_AGLO.eph_codagl\n",
    "radio_AGLO['NOMAGLO'] = radio_AGLO.eph_aglome\n",
    "\n",
    "radio_ref = radio_ref.drop(['AGLOMERADO'], axis = 1).merge(radio_AGLO[['radio','AGLOMERADO', 'NOMAGLO']], how = 'left')\n",
    "radio_ref['AGLOMERADO'] = radio_ref['AGLOMERADO'].fillna(0).astype(int)\n",
    "\n",
    "# Cargamos el mapeo de departamentos a regiones\n",
    "dpto_region = pd.read_csv('./../data/info/DPTO_PROV_Region.csv')\n",
    "radio_ref = radio_ref.merge(dpto_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radio_ref.dtypes/\n",
    "\n",
    "# Load rankings and map regions\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGLO_rk = pd.read_csv('./../../encuestador-de-hogares/data/info/AGLO_rk')\n",
    "rk_table = AGLO_rk.set_index(['ANO4', 'AGLOMERADO']).unstack()\n",
    "AGLO_rk_filled = rk_table.fillna(rk_table.mean()).stack().reset_index()\n",
    "AGLO_rk = AGLO_rk_filled\n",
    "\n",
    "Reg_rk = pd.read_csv('./../../encuestador-de-hogares/data/info/Reg_rk')\n",
    "# Reg_rk['Region'] = Reg_rk['region_']\n",
    "# Reg_rk = Reg_rk.drop('region_', axis=1)\n",
    "\n",
    "# # Define the mapping dictionary\n",
    "regiones = {\n",
    "    'Gran Buenos Aires': 'gran_buenos_aires',\n",
    "    'Pampeana': 'pampeana',\n",
    "    'Noroeste': 'noroeste',\n",
    "    'Noreste': 'noreste',\n",
    "    'Patagónica': 'patagonia',\n",
    "    'Cuyo': 'cuyo'\n",
    "}\n",
    "\n",
    "# Update region names using the mapping dictionary\n",
    "Reg_rk['Region'] = Reg_rk['Region'].map(regiones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = f'{REPO_PATH}data/censo_samples/table_f{FRAC}_{syr}_{EXPERIMENT_TAG}.csv'\n",
    "# pd.read_csv(file_path, nrows=1000).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = f'{REPO_PATH}data/censo_samples/table_f{FRAC}_{syr}_{EXPERIMENT_TAG}.csv'\n",
    "# input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-28 18:33:58] Script started. \n",
      "[2023-10-28 18:33:58] Processing year: 2004 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-28 18:39:29] Processing year: 2005 \n",
      "[2023-10-28 18:43:06] Processing year: 2006 \n",
      "[2023-10-28 18:47:14] Processing year: 2007 \n",
      "[2023-10-28 18:52:52] Processing year: 2008 \n",
      "[2023-10-28 18:58:59] Processing year: 2009 \n",
      "[2023-10-28 19:04:47] Processing year: 2010 \n",
      "[2023-10-28 19:10:59] Processing year: 2011 \n",
      "[2023-10-28 19:17:20] Processing year: 2012 \n",
      "[2023-10-28 19:23:53] Processing year: 2013 \n",
      "[2023-10-28 19:30:55] Processing year: 2014 \n",
      "[2023-10-28 19:37:24] Processing year: 2015 \n",
      "[2023-10-28 19:44:21] Processing year: 2016 \n",
      "[2023-10-28 19:51:39] Processing year: 2017 \n",
      "[2023-10-28 19:58:51] Processing year: 2018 \n",
      "[2023-10-28 20:07:42] Processing year: 2019 \n",
      "[2023-10-28 20:16:06] Processing year: 2020 \n",
      "[2023-10-28 20:23:22] Processing year: 2021 \n",
      "[2023-10-28 20:30:21] Processing year: 2022 \n",
      "[2023-10-28 20:38:29] Processing year: 2023 \n",
      "[2023-10-28 20:46:48] Script completed. \n"
     ]
    }
   ],
   "source": [
    "log_message(\"Script started.\")\n",
    "\n",
    "# Save the original working directory\n",
    "original_wd = os.getcwd()\n",
    "\n",
    "for yr in [s for s in range(START_YEAR, END_YEAR)]:\n",
    "    syr = str(yr)\n",
    "    log_message(f\"Processing year: {syr}\")\n",
    "\n",
    "    file_path = f'{REPO_PATH}data/censo_samples/table_f{FRAC}_{syr}_{EXPERIMENT_TAG}.csv'\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        log_message(f\"File {file_path} not found. Running the sampler script...\")\n",
    "        \n",
    "        # Navigate to the samplerCensoARG repository\n",
    "        os.chdir(os.path.join(os.getcwd(), REPO_PATH, \"notebooks\"))\n",
    "        \n",
    "        # Run the sampler script\n",
    "        # cmd = [\"python\", \"samplear.py\", \"-dbp\", DATABASE_PATH , \"-n\", EXPERIMENT_TAG, \"-f\", str(FRAC), \"-y\", str(yr)]\n",
    "        # subprocess.call(cmd)\n",
    "        cmd = [\"python\", \"samplear.py\", \"-dbp\", DATABASE_PATH, \"-n\", EXPERIMENT_TAG, \"-f\", str(FRAC), \"-y\", syr, str(yr+1)]\n",
    "        subprocess.run(cmd)\n",
    "        \n",
    "        # Navigate back to the original working directory\n",
    "        os.chdir(original_wd)\n",
    "        \n",
    "    # Extracción y transformación de datos\n",
    "    input_data = pd.read_csv(file_path)\n",
    "    # Extracción de datos\n",
    "    table = input_data.copy()\n",
    "    table = table.rename(columns = {'TOTPERS': 'IX_TOT'})\n",
    "    table['ANO4'] = int(yr)\n",
    "    \n",
    "    # Transformación de datos\n",
    "    table = transform_censo_data(table)\n",
    "    \n",
    "    # Agregar la región \n",
    "    table = table.merge(dpto_region[['DPTO', 'Region']])\n",
    "\n",
    "    # Generar IDs únicos\n",
    "    table = generate_unique_ids(table, 9)#, n_digits)\n",
    "\n",
    "    # Agregar ranking de Region y Aglo\n",
    "    table = table.merge(AGLO_rk[['AGLOMERADO', 'ANO4', 'AGLO_rk']]).merge(Reg_rk[['Region', 'ANO4', 'Reg_rk']])\n",
    "\n",
    "    # Guardado de los datos transformados\n",
    "    output_directory = '/media/matias/Elements/suite/poblaciones'\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    table.to_csv(f'{output_directory}/table_f{FRAC}_{syr}_{EXPERIMENT_TAG}.csv', index=False)\n",
    "\n",
    "log_message(\"Script completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # table.to_csv(f'{output_directory}/table_f{FRAC}_{syr}_{EXPERIMENT_TAG}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                   int64\n",
       "VIVIENDA_REF_ID      int64\n",
       "RADIO_REF_ID         int64\n",
       "TIPVV                int64\n",
       "V01                float64\n",
       "URP                  int64\n",
       "DPTO                 int64\n",
       "PROV                 int64\n",
       "AGLOMERADO           int64\n",
       "HOGAR_REF_ID         int64\n",
       "H05                  int64\n",
       "H06                float64\n",
       "H07                  int64\n",
       "H08                  int64\n",
       "H09                float64\n",
       "H10                  int64\n",
       "H11                  int64\n",
       "H12                  int64\n",
       "H13                float64\n",
       "H14                float64\n",
       "H15                  int64\n",
       "H16                  int64\n",
       "PROP                 int64\n",
       "IX_TOT               int64\n",
       "PERSONA_REF_ID       int64\n",
       "P01                  int64\n",
       "P02                  int64\n",
       "P03                  int64\n",
       "P05                  int64\n",
       "P06                  int64\n",
       "P07                  int64\n",
       "P12                  int64\n",
       "P08                  int64\n",
       "P09                  int64\n",
       "P10                  int64\n",
       "CONDACT              int64\n",
       "ANO4                 int64\n",
       "Region              object\n",
       "AGLO_rk            float64\n",
       "Reg_rk             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.dtypes\n",
    "\n",
    "# ID                   int64\n",
    "# VIVIENDA_REF_ID      int64\n",
    "# RADIO_REF_ID         int64\n",
    "# TIPVV                int64\n",
    "# V01                float64\n",
    "# URP                  int64\n",
    "# DPTO                 int64\n",
    "# PROV                 int64\n",
    "# AGLOMERADO           int64\n",
    "# HOGAR_REF_ID         int64\n",
    "# H05                  int64\n",
    "# H06                float64\n",
    "# H07                  int64\n",
    "# H08                  int64\n",
    "# H09                float64\n",
    "# H10                  int64\n",
    "# H11                  int64\n",
    "# H12                  int64\n",
    "# H13                float64\n",
    "# H14                float64\n",
    "# H15                  int64\n",
    "# H16                  int64\n",
    "# PROP                 int64\n",
    "# IX_TOT               int64\n",
    "# PERSONA_REF_ID       int64\n",
    "# P01                  int64\n",
    "# P02                  int64\n",
    "# P03                  int64\n",
    "# P05                  int64\n",
    "# P06                  int64\n",
    "# P07                  int64\n",
    "# P12                  int64\n",
    "# P08                  int64\n",
    "# P09                  int64\n",
    "# P10                  int64\n",
    "# CONDACT              int64\n",
    "# ANO4                 int64\n",
    "# Region              object\n",
    "# AGLO_rk            float64\n",
    "# Reg_rk             float64\n",
    "# dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
