{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads EPH data, cleans it and arranges it to be used as 'training sets'. \n",
    "That is, for fitting any Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startyr = 2003\n",
    "endyr = 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "# radio_ref[['PROV','NOMPROV','DPTO', 'NOMDPTO']].drop_duplicates().to_csv('./../data/DPTO_PROV.csv', index = False)\n",
    "dpto_region = pd.read_csv('./../data/info/DPTO_PROV_Region.csv')\n",
    "radio_ref = radio_ref.merge(dpto_region)\n",
    "AGLO_Region = radio_ref[['AGLOMERADO', 'Region']].drop_duplicates()\n",
    "\n",
    "# Decision sobre cual es la region de un aglomerado. GBA tiene que ir a Gran Buenos Aires, aunque algunos de sus radios en partidos como Rodriguez, Escobar, etc sean region pampeana.\n",
    "# Viedma Patagones, se tendria que tirar de un lado, y la mayoria de sus radios, son Patagonia.\n",
    "# Se tiene que corregir a mano, porque el AGLO 0 SI tiene varias regiones.\n",
    "\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 33) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 93) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "\n",
    "### Match column names\n",
    "\n",
    "names_censo = ['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO',\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']\n",
    "\n",
    "\n",
    "names_EPH = ['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']\n",
    "\n",
    "col_mon = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi = pd.read_csv('./../data/info/indice_precios_Q.csv', index_col=0)\n",
    "\n",
    "cpi.index = pd.to_datetime(cpi.index)\n",
    "cpi = cpi['2003':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>3188.361836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-31</th>\n",
       "      <td>3270.799721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  index\n",
       "2020-12-31  3188.361836\n",
       "2021-01-31  3270.799721"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpi_M = pd.read_csv('./../data/info/indice_precios_M.csv', index_col=0)\n",
    "cpi_mes_actual = cpi_M.iloc[-1][0]\n",
    "\n",
    "cpi_M.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270.7997208542934"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpi_mes_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import MonthEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPHs\n",
    "\n",
    "Los microdatos de la Encuesta Permanente de Hogares se pueden descargar con:\n",
    "\n",
    "``git clone https://github.com/matuteiglesias/microdatos-EPH-INDEC.git``\n",
    "\n",
    "(darle star al repositorio)\n",
    "\n",
    "tomando pull del mismo repositorio se va a poder actualizar con los nuevos microdatos a medida que se publican. Siempre y cuando el repositorio se mantenga actualizado. Y hasta que no se supere la capacidad de almacenamiento en repo github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "13180\n",
      "13325\n",
      "(26483, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t303.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t403.txt\n",
      "(93244, 33)\n",
      "Hogar - Indiv merged:\n",
      "(96827, 49)\n",
      "No aglo agregado:\n",
      "(193654, 50)\n",
      "deflactado:\n",
      "(193654, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21         9601.642114\n",
       "P47T       14257.916149\n",
       "PP08D1      6742.703017\n",
       "TOT_P12      690.324496\n",
       "T_VI        3358.731077\n",
       "V12_M        326.712745\n",
       "V2_M        2178.890000\n",
       "V3_M          85.050709\n",
       "V5_M         181.481508\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004\n",
      "12816\n",
      "13809\n",
      "13806\n",
      "13497\n",
      "(53873, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t104.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t204.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t304.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t404.txt\n",
      "(187930, 33)\n",
      "Hogar - Indiv merged:\n",
      "(193742, 49)\n",
      "No aglo agregado:\n",
      "(387484, 50)\n",
      "deflactado:\n",
      "(387484, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21        10420.817954\n",
       "P47T       14858.476407\n",
       "PP08D1      7447.722074\n",
       "TOT_P12      759.931275\n",
       "T_VI        3142.787202\n",
       "V12_M        430.271753\n",
       "V2_M        2029.606590\n",
       "V3_M          61.431053\n",
       "V5_M         126.052947\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n",
      "13597\n",
      "13511\n",
      "13807\n",
      "13704\n",
      "(54562, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t105.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t205.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t305.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t405.txt\n",
      "(188755, 33)\n",
      "Hogar - Indiv merged:\n",
      "(194504, 49)\n",
      "No aglo agregado:\n",
      "(389008, 50)\n",
      "deflactado:\n",
      "(389008, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21        11850.320960\n",
       "P47T       16813.927935\n",
       "PP08D1      8533.339525\n",
       "TOT_P12      855.435883\n",
       "T_VI        3494.079453\n",
       "V12_M        472.298318\n",
       "V2_M        2259.660747\n",
       "V3_M          55.608707\n",
       "V5_M         132.208392\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "13315\n",
      "13962\n",
      "18866\n",
      "18655\n",
      "(64739, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t106.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t206.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t306.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t406.txt\n",
      "(222968, 33)\n",
      "Hogar - Indiv merged:\n",
      "(229210, 49)\n",
      "No aglo agregado:\n",
      "(458420, 50)\n",
      "deflactado:\n",
      "(458420, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21        14159.626971\n",
       "P47T       19789.582667\n",
       "PP08D1     10363.685419\n",
       "TOT_P12      987.493373\n",
       "T_VI        3869.428559\n",
       "V12_M        522.154745\n",
       "V2_M        2484.659243\n",
       "V3_M          60.579966\n",
       "V5_M         152.481131\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007\n",
      "18360\n",
      "18526\n",
      "17891\n",
      "(54739, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t107.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t207.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t407.txt\n",
      "(188433, 33)\n",
      "Hogar - Indiv merged:\n",
      "(192995, 49)\n",
      "No aglo agregado:\n",
      "(385990, 50)\n",
      "deflactado:\n",
      "(385990, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21        15323.390155\n",
       "P47T       21164.806011\n",
       "PP08D1     11449.267706\n",
       "TOT_P12      991.577212\n",
       "T_VI        4263.825192\n",
       "V12_M        490.114272\n",
       "V2_M        2870.209130\n",
       "V3_M          70.431555\n",
       "V5_M         164.128827\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008\n",
      "18123\n",
      "18224\n",
      "18326\n",
      "18217\n",
      "(72829, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t108.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t208.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t308.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t408.txt\n",
      "(247819, 33)\n",
      "Hogar - Indiv merged:\n",
      "(253536, 49)\n",
      "No aglo agregado:\n",
      "(507072, 50)\n",
      "deflactado:\n",
      "(507072, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21        15838.565131\n",
       "P47T       22204.793446\n",
       "PP08D1     12014.792822\n",
       "TOT_P12      995.909228\n",
       "T_VI        4551.234846\n",
       "V12_M        494.838208\n",
       "V2_M        3052.198879\n",
       "V3_M         115.051563\n",
       "V5_M         147.039994\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n",
      "17820\n",
      "17904\n",
      "18149\n",
      "17807\n",
      "(71622, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t109.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t209.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t309.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual\\usu_individual_t409.txt\n",
      "(240967, 33)\n",
      "Hogar - Indiv merged:\n",
      "(246169, 49)\n",
      "No aglo agregado:\n",
      "(492338, 50)\n",
      "deflactado:\n",
      "(492338, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21        16241.096454\n",
       "P47T       22743.130723\n",
       "PP08D1     12513.971467\n",
       "TOT_P12      993.413570\n",
       "T_VI        4670.953654\n",
       "V12_M        500.143759\n",
       "V2_M        3230.774521\n",
       "V3_M         120.372005\n",
       "V5_M         150.476299\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path ='./../../microdatos-EPH-INDEC/microdatos/' # depende de donde hayamos descargado los microdatos\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    allFiles = glob.glob(path + 'hogar/*'+str(yr)+'.txt')\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                        usecols = ['CODUSU','ANO4','TRIMESTRE','IX_TOT', 'AGLOMERADO',\n",
    "        'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9']) \n",
    "        ['II2', 'IV5', 'IX_TOT', 'II7', 'IV4', 'II1', 'IV7', 'IV6', 'IV11', 'IV8', 'IV3', 'II8', 'IV1', 'IV10', 'II9']\n",
    "        \n",
    "        print(len(df))\n",
    "        list_ += [df]\n",
    "    df = pd.concat(list_)\n",
    "\n",
    "    # Correcciones Respuestas. Para que matchee censo\n",
    "    df = df.loc[df.IV1 != 9]\n",
    "    df['IV10'] = df['IV10'].map({1: 1, 2: 2, 3: 2, 0: 0, 9: 9})\n",
    "    df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "    df['II7'] = df['II7'].map({1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 6, 8: 6, 9: 6, 0: 0})\n",
    "    df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "    df['IX_TOT'] = df['IX_TOT'].clip(0, 8)\n",
    "    \n",
    "    hogar = df\n",
    "    hogar = hogar.drop_duplicates()\n",
    "    print(hogar.shape)\n",
    "\n",
    "    allFiles = glob.glob(path + 'individual/usu_individual*'+str(yr)+'.txt')\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        print(file_)\n",
    "    #     print(file_)\n",
    "        df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                         usecols = ['CODUSU','ANO4','TRIMESTRE','CH04','CH06', 'AGLOMERADO', 'CH09','CH10','CH12','CH13','CH15'] +\\\n",
    "                         ['CH07', 'ESTADO','CAT_INAC','CAT_OCUP','PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K',\n",
    "                         'P47T', 'V3_M', 'T_VI', 'V12_M', 'TOT_P12', 'V5_M','V2_M', 'PP08D1', 'P21'])\n",
    "        df = df.rename(columns = {'ESTADO': 'CONDACT'})\n",
    "\n",
    "#         display(df.head())\n",
    "# revisar estado, condact, cat ocup, cat inac.\n",
    "    # For the regression training set. But for these the ANO4 TRIMESTRE is important.. Also we need more memory.\n",
    "    #                      ['P21','P47T',,'CH08','CH16','TOT_P12','T_VI','V10_M','V11_M','V12_M','V18_M','V19_AM','V21_M','V2_M','V3_M',\n",
    "    #             'V4_M','V5_M','V8_M','V9_M','PP08D1','PP08D4','PP08F1','PP08F2','PP08J1','PP08J2','PP08J3','PP10A','PP10C','PP10D','PP10E'])\n",
    "#         print(len(df))\n",
    "        list_ += [df]\n",
    "    df = pd.concat(list_)\n",
    "\n",
    "    # Correcciones Respuestas. Para que matchee censo\n",
    "    df['CH15'] = df['CH15'].map({1:1, 2:1, 3:1, 4:2, 5:2, 9:0})\n",
    "    df['CH06'] = df['CH06'].clip(0)\n",
    "    df['CH09'] = df['CH09'].map({1:1, 2:2, 0:2, 3:2})\n",
    "    df.loc[df['CH06'] < 14, 'CONDACT'] = 0 # Menores de 14 van con CONDACT 0, como en el Censo\n",
    "    \n",
    "    ## En Censo, Jardin y educacion especial no preguntan terminado si/no.\n",
    "    df['CH12'] = df.CH12.replace(99, 0)\n",
    "    df.loc[df.CH12.isin([0, 1, 9]), 'CH13'] = 0\n",
    "\n",
    "#     df['MAYOR'] = df['CH06'] >= 14 \n",
    "#     df['MAYOR'] = df['CH06'] // 7\n",
    "#     df['CONDACT'] = df['CAT_OCUP'].fillna(-1)\n",
    "\n",
    "    indiv = df\n",
    "    indiv = indiv.dropna(subset = ['P47T'])\n",
    "    print(indiv.shape)\n",
    "\n",
    "    indiv_table = indiv[list(indiv.columns.difference(hogar.columns)) + ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO']]\n",
    "    EPH = hogar.merge(indiv_table, on = ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO'], indicator = True)\n",
    "\n",
    "    print('Hogar - Indiv merged:')\n",
    "    print(EPH.shape)\n",
    "\n",
    "    \n",
    "#     EPH = EPH.loc[EPH.P47T != -9]\n",
    "    \n",
    "    EPH = EPH.merge(AGLO_Region)\n",
    "\n",
    "    EPH_no_aglo = EPH.copy(); \n",
    "    EPH_no_aglo['AGLOMERADO'] = 0\n",
    "\n",
    "    EPH = pd.concat([EPH, EPH_no_aglo]).reset_index(drop = True)\n",
    "\n",
    "    print('No aglo agregado:')\n",
    "    print(EPH.shape)\n",
    "    \n",
    "    # Quarters / deflation\n",
    "    EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "    EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') + MonthEnd(1)\n",
    "#     cpi_ultimo_Q = indice_precios['index'].values[-1]\n",
    "    \n",
    "    EPH[col_mon] = cpi_mes_actual*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "    \n",
    "    # 2018Q3 -> Mar19 1.3156\n",
    "    # 2018Q3 -> Abr19 1.361\n",
    "#     EPH[col_mon] = 1.361*EPH[col_mon]\n",
    "    \n",
    "    EPH[col_mon] = EPH[col_mon].round()\n",
    "    \n",
    "    print('deflactado:')\n",
    "    print(EPH.shape)\n",
    "    display(EPH[col_mon].mean())\n",
    "    \n",
    "    \n",
    "    training = EPH.rename(columns = dict(zip(names_EPH, names_censo)))\n",
    "    \n",
    "    if not os.path.exists('./../data/training/'):\n",
    "        os.makedirs('./../data/training/')\n",
    "    \n",
    "    training.to_csv('./../data/training/EPHARG_train_'+str(yr)+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de AGLOS y Regiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "# for yr in [str(s) for s in [2006, 2011, 2016]]:\n",
    "for yr in [str(s) for s in [2015, 2020]]:\n",
    "# for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "    print(yr)\n",
    "    train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')\n",
    "    train = train.loc[train.P47T >= -0.001].fillna(0)#.sample(400000)\n",
    "    df_list += [train]\n",
    "    \n",
    "train_df = pd.concat(df_list)\n",
    "\n",
    "AGLO_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['AGLOMERADO'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'AGLO_rk'})\n",
    "Reg_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['Region'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'Reg_rk'})\n",
    "\n",
    "AGLO_rk['AGLO_rk'] = AGLO_rk.AGLO_rk/AGLO_rk.AGLO_rk.max()\n",
    "AGLO_rk.to_csv('./../data/info/AGLO_rk', index = False)\n",
    "Reg_rk['Reg_rk'] = Reg_rk.Reg_rk/Reg_rk.Reg_rk.max()\n",
    "Reg_rk.to_csv('./../data/info/Reg_rk', index = False)\n",
    "\n",
    "# check it out\n",
    "# AGLO_rk.merge(pd.read_csv('./../data/info/aglo_labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n"
     ]
    }
   ],
   "source": [
    "AGLO_rk = pd.read_csv('./../data/info/AGLO_rk')\n",
    "Reg_rk = pd.read_csv('./../data/info/Reg_rk')\n",
    "\n",
    "df_list = []\n",
    "for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "# for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "    print(yr)\n",
    "    train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')#.drop(['AGLO_rk', 'Reg_rk'], axis = 1)\n",
    "    train = train.loc[train.P47T >= -0.001].fillna(0)\n",
    "    train = train.merge(AGLO_rk[['AGLOMERADO', 'AGLO_rk']]).merge(Reg_rk[['Region', 'Reg_rk']])\n",
    "    train.to_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listo. Salvado el training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borradores sobre los nombres de columnas.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Misma info, distinto nombre. \n",
    "# # Censo INDEC \n",
    "# md_1 = table[['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO', #las que no se erran, cant pers, sexo, edad, act, aglo\n",
    "#     'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "#       'P07', 'P08', 'P09', 'P10', 'P05']] #las x que buscan matches un poquito mas laxamente\n",
    "\n",
    "\n",
    "# #Mismas cosas, distinto nombre de columna para\n",
    "# # EPH INDEC\n",
    "# md_2 = EPH[['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "#     'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "#     'CH09','CH10','CH12','CH13','CH15']]\n",
    "\n",
    "# # # Now we want to see in each column what are the percentages, as a clue to where there can be issues\n",
    "# # # OK control check. Control there is less likely confusion. \n",
    "\n",
    "# # for i in range(len(md_1.columns))[:2]: \n",
    "# #     print('\\n')\n",
    "# #     for md in [md_1, md_2]:\n",
    "# #         col = md.columns[i]\n",
    "# #         print(col)\n",
    "# #         print(md[col].value_counts().sort_index()/len(md))\n",
    "\n",
    "# md_2.columns = md_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # Agregar $$$. En millones de usd\n",
    "# # En millones de usd (USD = 30 ARS)\n",
    "# _USD = 30.5 #ARS\n",
    "# np.round(res_1.sum()/_USD/1e6, 1).sort_values().tail(14)\n",
    "\n",
    "# #PPALES\n",
    "# # negocio que no trabajo no laborable (V9_M)\n",
    "# # alquiler no laborable (V8_M)\n",
    "# # indemnizacion despido no laborable (V3_M)\n",
    "# # comision Ocupacion ppal (PP08F1)\n",
    "# # cuota alimentos no laborable (V12_M)\n",
    "# # subsidio ayuda social no laborable (V5_M)\n",
    "# # TOTAL otras ocupacions(TOT_P12)\n",
    "# # jubilacion no laborable (V2_M)\n",
    "# # TOTAL no laborables (T_VI)\n",
    "# # sueldo Ocupacion ppal(PP08D1)\n",
    "# # TOTAL Ocupacion ppal (P21)\n",
    "# # TOTAL TOTAL (P47T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### otros borradores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.round(res_1.sum()/1e6/_USD, 1).sort_values().tail(14).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PERS_DPTO = table[['PERSONA_REF_ID', 'RADIO_REF_ID']].merge(radio_ref_sel[['RADIO_REF_ID', 'DPTO' #, 'NOMDPTO', 'NOMPROV'\n",
    "#                                                                       ]]).drop(['RADIO_REF_ID'], axis = 1)\n",
    "\n",
    "# res = res_1\n",
    "# res_DPTO = res.merge(PERS_DPTO, on = 'PERSONA_REF_ID')\n",
    "\n",
    "# #en miles de USD\n",
    "# res_byDPTO = res_DPTO.groupby(['DPTO'])[[  'V3_M', 'V12_M', 'V5_M', 'TOT_P12',\n",
    "#        'V2_M', 'T_VI', 'PP08D1', 'P21', 'P47T']].sum()\n",
    "\n",
    "# np.round(100*res_byDPTO.div(res_byDPTO.P47T, axis = 0), 1).sort_values(by = 'P21').head() #percentage\n",
    "# # np.round(res_byDPTO/1e3/_USD, 1) #in 1000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERS_DPTO = table[['PERSONA_REF_ID', 'RADIO_REF_ID']].merge(radio_ref_sel[['RADIO_REF_ID', 'DPTO', 'NOMDPTO', 'NOMPROV']]\n",
    "#                                                            )#.drop(['RADIO_REF_ID'], axis = 1)\n",
    "\n",
    "# res = res_1\n",
    "# res_DPTO = res.merge(PERS_DPTO, on = 'PERSONA_REF_ID')\n",
    "\n",
    "# # variables = ['V9_M', 'V8_M', 'PP08F1', 'V3_M', 'V12_M', 'V5_M', 'TOT_P12',\n",
    "# #        'V2_M', 'T_VI', 'PP08D1', 'P21', 'P47T']\n",
    "# variables = [  'V3_M', 'V12_M', 'V5_M', 'TOT_P12',\n",
    "#        'V2_M', 'T_VI', 'PP08D1', 'P21', 'P47T']\n",
    "# #en miles de USD\n",
    "# # res_byDPTO = res_DPTO.groupby(['DPTO', 'NOMDPTO', 'NOMPROV'])[variables].sum()\n",
    "# res_byDPTO = res_DPTO.groupby(['RADIO_REF_ID'])[variables].sum()\n",
    "\n",
    "# # np.round(100*res_byDPTO.div(res_byDPTO.P47T, axis = 0), 1).sort_values(by = 'P21').head() #percentage\n",
    "# np.round(res_byDPTO/1e3/_USD, 1) #in 1000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Save info at 'radio' level\n",
    "# res_byDPTO.to_csv('res_byradio_sample_'+str(n).zfill(3)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Desoc, NA = 0. Not good.\n",
    "# variables = ['PP07J', #turno habitual\n",
    "#  'PP10D', #Desoc. Ha trabajado alguna vez?\n",
    "#  'PP10C', #Desoc. Hizo changa mientras buscaba?\n",
    "#  'PP07K', # Oc. ppal. Inc. serv. dom. Cobra con recibo\n",
    "#  'PP07G2', # Oc. ppal. Inc. serv. dom. aguinaldo\n",
    "#  'PP07G_59', # Oc. ppal. Inc. serv. dom. ninguno\n",
    "#  'PP07G3', # Oc. ppal. Inc. serv. dom. dias enfermedad\n",
    "#  'PP10E', # Desoc. Tiempo de que termino su ultimo trabajo/changa\n",
    "#  'PP07H', # Oc. ppal. Inc. serv. dom. descuento jubilatorio\n",
    "#  'PP07G4', # Oc. ppal. Inc. serv. dom. obra social\n",
    "#  'PP07I', # Oc. ppal. Inc. serv. dom. Aporta jub por s√≠ mismo \n",
    "#  'PP07G1', # Oc. ppal. Inc. serv. dom. vacaciones pagas\n",
    "#  'CH07'] #Est civil\n",
    "\n",
    "# #en miles de USD\n",
    "# # res_byDPTO = res_DPTO.groupby(['DPTO', 'NOMDPTO', 'NOMPROV'])[variables].mean()\n",
    "# res_byDPTO = res_DPTO.groupby(['RADIO_REF_ID'])[variables].mean()\n",
    "\n",
    "\n",
    "# s = np.round(res_byDPTO, 2).sort_values(by = 'PP07K')#.head() \n",
    "# s.style.bar(color='#d65f5f')\n",
    "# #  'CAT_OCUP', #CAT_INAC\n",
    "# #  'CAT_INAC', #CAT_INAC\n",
    "\n",
    "# #  'CH08', obra social/salud. nums altos como para mean\n",
    "# # 'CH16', # Donde vivia hace 5. Desconfiable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# for col in result.columns:\n",
    "#     print('\\n')\n",
    "#     print(col)\n",
    "#     df_ = result.loc[result.P03 > 2]\n",
    "#     print(df_[col].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
