{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos individuales a estadisticas agregadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## En esta notebook estan ...\n",
    "\n",
    "# El universo (base) de analisis pueden ser:\n",
    "#     - [P] Personas\n",
    "#     - [H] Hogares\n",
    "#     - [M] Mayores de 24\n",
    "#     - [Hp] Hogares en pobreza\n",
    "#     - [Hi] Hogares en indigencia\n",
    "\n",
    "# Ademas restringido a un espacio geografico:\n",
    "#     - ['AMBA'] i.e. data.loc[data.AGLOMERADO.isin([33, 32, 2])]\n",
    "\n",
    "# Las variables agrupadoras pueden ser: \n",
    "#     - ['Q', 'Grupo Etario']\n",
    "#     - ['Q','Total pais']\n",
    "#     - ['Q','AGLOMERADO']\n",
    "#     - ['Q','PROV']\n",
    "#     - ['Q','AGLO_si']\n",
    "#     - ['Q','P0910']\n",
    "#     - ['Q','DPTO']\n",
    "#     - ['Q','Region']\n",
    "\n",
    "\n",
    "# Las variables de resultado pueden ser: (variable, sintetico, base)\n",
    "#     - [\"Pobreza\", \"mean\", \"Personas\"]\n",
    "#     - [\"Pobreza\", \"sum\", \"Personas\"]\n",
    "#     - [\"Indigencia\", \"mean\", \"Personas\"]\n",
    "#     - [\"Indigencia\", \"sum\", \"Personas\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def sintetizar(data, grouper, base='Personas', frac=0.05):\n",
    "    df = data.copy()\n",
    "    df['Total'] = True\n",
    "    df['timestamp'] = dt.datetime.today()\n",
    "    df['AGLO_si'] = df.AGLOMERADO != 0\n",
    "    df['Total_pais'] = True\n",
    "\n",
    "    # Columnas comunes\n",
    "    columns_to_groupby = ['Total', 'Pobreza', 'Indigencia']\n",
    "    \n",
    "    # Columnas específicas según base\n",
    "    if 'P47T_persona' in df.columns:\n",
    "        columns_to_groupby.append('P47T_persona')\n",
    "    if 'P47T_hogar' in df.columns:\n",
    "        columns_to_groupby.append('P47T_hogar')\n",
    "    if 'CB_EQUIV' in df.columns:\n",
    "        columns_to_groupby.extend(['CB_EQUIV', 'CBA', 'gap_indigencia', 'CBT', 'gap_pobreza'])\n",
    "\n",
    "    agg_dict = {\n",
    "        'Total': ['mean', 'sum'],\n",
    "        'Pobreza': ['mean', 'sum'],\n",
    "        'Indigencia': ['mean', 'sum']\n",
    "    }\n",
    "    \n",
    "    if 'P47T_persona' in df.columns:\n",
    "        agg_dict['P47T_persona'] = ['mean', q10, q25, 'median', q75, q90]\n",
    "    if 'P47T_hogar' in df.columns:\n",
    "        agg_dict['P47T_hogar'] = ['mean', q10, q25, 'median', q75, q90]\n",
    "    if 'CB_EQUIV' in df.columns:\n",
    "        agg_dict['CB_EQUIV'] = ['mean', 'median']\n",
    "        agg_dict['CBA'] = ['mean', 'median']\n",
    "        agg_dict['gap_indigencia'] = ['mean', 'median']\n",
    "        agg_dict['CBT'] = ['mean', 'median']\n",
    "        agg_dict['gap_pobreza'] = ['mean', 'median']\n",
    "\n",
    "    df = df.groupby(grouper + ['timestamp'])[columns_to_groupby].agg(agg_dict)\n",
    "    \n",
    "    for col in ['Total', 'Pobreza', 'Indigencia']:\n",
    "        df[(col, 'sum')] = (df[(col, 'sum')]/frac).round(1)\n",
    "        df[(col, 'mean')] = df[(col, 'mean')].round(4)\n",
    "\n",
    "    if 'P47T_persona' in df.columns:\n",
    "        df['P47T_persona'] = df['P47T_persona'].round(-1).astype(int)\n",
    "    if 'P47T_hogar' in df.columns:\n",
    "        df['P47T_hogar'] = df['P47T_hogar'].round(-1).astype(int)\n",
    "\n",
    "    agg_result = df.T.set_index(np.repeat(base, df.shape[1]), append=True)\n",
    "    stacker_ix = [-i for i in range(len(grouper) + 1)]\n",
    "    agg_result = agg_result.stack(level=stacker_ix).reset_index()\n",
    "    \n",
    "    agg_result = agg_result.rename(columns = {'level_0': 'observable', 'level_1': 'sintetico', 'level_2': 'base', 0: 'valor'})\n",
    "    agg_result['frac'] = frac\n",
    "    return agg_result\n",
    "\n",
    "# Funciones de percentil\n",
    "def q10(x):\n",
    "    return x.quantile(0.1)\n",
    "\n",
    "def q25(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def q75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "def q90(x):\n",
    "    return x.quantile(0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de info_personas: Index(['ID', 'HOGAR_REF_ID', 'Q', 'P02', 'P03', 'P0910', 'P47T_persona',\n",
      "       'P47T_hogar', 'CBA', 'CBT', 'CB_EQUIV', 'Pobreza', 'Indigencia',\n",
      "       'gap_pobreza', 'gap_indigencia', 'RADIO_REF_ID', 'AGLOMERADO',\n",
      "       'FRAC_REF_ID', 'DPTO', 'NOMDPTO', 'PROV', 'NOMPROV', 'Region',\n",
      "       'COD_2010', 'distrito_id', 'seccion_id', 'seccion_nombre', 'circuito',\n",
      "       'IN1', 'NAM'],\n",
      "      dtype='object')\n",
      "\n",
      "Columnas de info_hogares: Index(['HOGAR_REF_ID', 'Q', 'P47T_hogar', 'CBA', 'CBT', 'CB_EQUIV', 'Pobreza',\n",
      "       'Indigencia', 'gap_pobreza', 'gap_indigencia', 'RADIO_REF_ID',\n",
      "       'AGLOMERADO', 'FRAC_REF_ID', 'DPTO', 'NOMDPTO', 'PROV', 'NOMPROV',\n",
      "       'Region', 'COD_2010', 'distrito_id', 'seccion_id', 'seccion_nombre',\n",
      "       'circuito', 'IN1', 'NAM'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Parámetros\n",
    "frac = 0.05\n",
    "Q = '2023-02-15'\n",
    "experiment_tag = 'ARG'  # Asumiendo que esta es la etiqueta, según tu ejemplo\n",
    "\n",
    "# Nombres de los archivos\n",
    "personas_ingresos_Q_file = f'./../data/Pobreza/personas_ingresos_f{frac}_{Q}_{experiment_tag}.csv'\n",
    "pobreza_hogares_file = f'./../data/Pobreza/pobreza_hogares_f{frac}_q{Q}.csv'\n",
    "hogares_geo_file = f'./../data/Pobreza/hogares_geo_f{frac}_{Q.split(\"-\")[0]}_{experiment_tag}.csv'\n",
    "\n",
    "# Cargar los archivos (solo las primeras 5 filas)\n",
    "personas_ingresos_Q = pd.read_csv(personas_ingresos_Q_file, nrows=5)\n",
    "pobreza_hogares = pd.read_csv(pobreza_hogares_file, nrows=5)\n",
    "hogares_geo = pd.read_csv(hogares_geo_file, nrows=5)\n",
    "\n",
    "# Merges\n",
    "info_personas = personas_ingresos_Q.merge(pobreza_hogares, on=['HOGAR_REF_ID', 'Q'], how='left').merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "info_hogares = pobreza_hogares.merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "\n",
    "# Mostrar columnas\n",
    "print(\"Columnas de info_personas:\", info_personas.columns)\n",
    "print(\"\\nColumnas de info_hogares:\", info_hogares.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Parámetros\n",
    "frac = 0.05\n",
    "experiment_tag = 'ARG'\n",
    "\n",
    "# Ruta de los datos\n",
    "path = './../data/Pobreza/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobar y crear el directorio de resultados si no existe\n",
    "results_path = './../data/results'\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "def add_to_json(synth_func, data, grouper, base_str, frac):\n",
    "    \"\"\"Función para agregar datos a un archivo JSON.\"\"\"\n",
    "    filename = f'{results_path}/result_{base_str}_{\"-\".join(grouper)}_{frac}.json'\n",
    "    \n",
    "    df = synth_func(data, grouper, frac)\n",
    "    \n",
    "    # Si el archivo no existe, lo crea. Si existe, concatena los nuevos datos.\n",
    "    if not os.path.exists(filename):\n",
    "        df.to_json(filename, orient=\"records\")\n",
    "    else:\n",
    "        df_ = pd.concat([df, pd.read_json(filename)])\n",
    "        df_.to_json(filename, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando fecha Q: 2022-05-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1411810/2818111489.py:21: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  hogares_geo = pd.read_csv(hogares_geo_file)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'P0910'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m add_to_json(sintetizar, info_personas[info_personas\u001b[39m.\u001b[39mP03 \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m24\u001b[39m], grouper, base_str\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mM24\u001b[39m\u001b[39m'\u001b[39m, frac\u001b[39m=\u001b[39mfrac)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Sintetizar para hogares\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m add_to_json(sintetizar, info_hogares, grouper, base_str\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mH\u001b[39;49m\u001b[39m'\u001b[39;49m, frac\u001b[39m=\u001b[39;49mfrac)\n\u001b[1;32m     40\u001b[0m add_to_json(sintetizar, info_hogares[info_hogares\u001b[39m.\u001b[39mPobreza], grouper, base_str\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mHp\u001b[39m\u001b[39m'\u001b[39m, frac\u001b[39m=\u001b[39mfrac)\n\u001b[1;32m     41\u001b[0m add_to_json(sintetizar, info_hogares[info_hogares\u001b[39m.\u001b[39mIndigencia], grouper, base_str\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mHi\u001b[39m\u001b[39m'\u001b[39m, frac\u001b[39m=\u001b[39mfrac)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36madd_to_json\u001b[0;34m(synth_func, data, grouper, base_str, frac)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Función para agregar datos a un archivo JSON.\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mresults_path\u001b[39m}\u001b[39;00m\u001b[39m/result_\u001b[39m\u001b[39m{\u001b[39;00mbase_str\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(grouper)\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mfrac\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m df \u001b[39m=\u001b[39m synth_func(data, grouper, frac)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Si el archivo no existe, lo crea. Si existe, concatena los nuevos datos.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(filename):\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36msintetizar\u001b[0;34m(data, grouper, base, frac)\u001b[0m\n\u001b[1;32m     35\u001b[0m     agg_dict[\u001b[39m'\u001b[39m\u001b[39mCBT\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m     agg_dict[\u001b[39m'\u001b[39m\u001b[39mgap_pobreza\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mgroupby(grouper \u001b[39m+\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m])[columns_to_groupby]\u001b[39m.\u001b[39magg(agg_dict)\n\u001b[1;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPobreza\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIndigencia\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     41\u001b[0m     df[(col, \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)] \u001b[39m=\u001b[39m (df[(col, \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)]\u001b[39m/\u001b[39mfrac)\u001b[39m.\u001b[39mround(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/core/frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   8250\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8252\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   8253\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   8254\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[1;32m   8255\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   8256\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   8257\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[1;32m   8258\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   8259\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[1;32m   8260\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m   8261\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[1;32m   8262\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropna \u001b[39m=\u001b[39m dropna\n\u001b[1;32m    930\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[1;32m    932\u001b[0m         obj,\n\u001b[1;32m    933\u001b[0m         keys,\n\u001b[1;32m    934\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    935\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    936\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    937\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m    938\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[1;32m    939\u001b[0m     )\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[1;32m    942\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m    983\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    986\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'P0910'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lista de agrupaciones para las transformaciones\n",
    "# groupers = [['Q', 'AGLO_si']]\n",
    "groupers = [['Q','Total_pais'], ['Q','AGLO_si'], ['Q','AGLOMERADO'], ['Q','Region'], ['Q','Region', 'AGLO_si'], \n",
    "            ['Q', 'Region', 'AGLOMERADO'], ['Q','PROV'], ['Q','DPTO'], ['Q', 'PROV', 'AGLO_si'], ['Q','P0910']]\n",
    "\n",
    "# Lista de fechas Q que se van a procesar\n",
    "Qs = ['2022-05-15', '2022-08-15', '2022-11-15', '2023-02-15']  # Aquí puedes agregar más fechas si es necesario\n",
    "\n",
    "# Bucle para procesar cada valor de Q\n",
    "for Q in Qs:\n",
    "    print(f\"Procesando fecha Q: {Q}\")\n",
    "    \n",
    "    # Nombres de los archivos dependientes de Q\n",
    "    personas_ingresos_Q_file = f'{path}personas_ingresos_f{frac}_{Q}_{experiment_tag}.csv'\n",
    "    pobreza_hogares_file = f'{path}pobreza_hogares_f{frac}_q{Q}.csv'\n",
    "    hogares_geo_file = f'{path}hogares_geo_f{frac}_{Q.split(\"-\")[0]}_{experiment_tag}.csv'\n",
    "\n",
    "    # Cargar los archivos\n",
    "    personas_ingresos_Q = pd.read_csv(personas_ingresos_Q_file)\n",
    "    pobreza_hogares = pd.read_csv(pobreza_hogares_file)\n",
    "    hogares_geo = pd.read_csv(hogares_geo_file)\n",
    "\n",
    "    # Fusiones para obtener los datasets consolidados\n",
    "    info_personas = personas_ingresos_Q.merge(pobreza_hogares, on=['HOGAR_REF_ID', 'Q'], how='left').merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "    info_hogares = pobreza_hogares.merge(hogares_geo, on='HOGAR_REF_ID', how='left')\n",
    "\n",
    "    # Agregar AGLO SI, y Total Pais.\n",
    "    info_personas['AGLO_si'] = info_personas.AGLOMERADO != 0; info_personas['Total_pais'] = True\n",
    "    info_hogares['AGLO_si'] = info_hogares.AGLOMERADO != 0; info_hogares['Total_pais'] = True\n",
    "\n",
    "    # Aplicar transformaciones y guardar resultados\n",
    "    for grouper in groupers:\n",
    "        # Sintetizar para personas\n",
    "        add_to_json(sintetizar, info_personas, grouper, base_str='P', frac=frac)\n",
    "        add_to_json(sintetizar, info_personas[info_personas.AGLO_si], grouper, base_str='PAGLO', frac=frac)\n",
    "        add_to_json(sintetizar, info_personas[info_personas.P03 >= 24], grouper, base_str='M24', frac=frac)\n",
    "        \n",
    "        # Sintetizar para hogares\n",
    "        add_to_json(sintetizar, info_hogares, grouper, base_str='H', frac=frac)\n",
    "        add_to_json(sintetizar, info_hogares[info_hogares.Pobreza], grouper, base_str='Hp', frac=frac)\n",
    "        add_to_json(sintetizar, info_hogares[info_hogares.Indigencia], grouper, base_str='Hi', frac=frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# frac\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xx\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xx' is not defined"
     ]
    }
   ],
   "source": [
    "# frac\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_str = 'H'\n",
    "for grouper in groupers:\n",
    "    file = './../data/results/result_'+base_str+'_'+'-'.join(grouper)+'.json'\n",
    "    print(file)\n",
    "    info = pd.read_json(file).drop('timestamp', axis = 1).drop_duplicates()\n",
    "\n",
    "    info['Q'] = pd.to_datetime(info['Q'])\n",
    "\n",
    "    # Mean pobreza, indigencia\n",
    "    info_ = info.loc[info.sintetico == 'mean'].set_index(['observable'] + grouper)['valor']\n",
    "#     plot_data = info_.unstack([1]).T\n",
    "\n",
    "    ## Columnas que no son ni el trimestre ni el valor. Identifican las series de tiempo.\n",
    "    time_series_cols = list(col for col in info.columns if col not in ['Q', 'valor'])\n",
    "    plot_data = info.groupby([pd.Grouper(freq = '6M', key='Q')] + time_series_cols).agg({'valor': 'mean'}).unstack(0)['valor'].T\n",
    "    \n",
    "    if len(plot_data) < 50:\n",
    "        fig, axs = plt.subplots(1, figsize = (6, 3.5))\n",
    "#         plot_data.plot(ax = axs, marker = '.')\n",
    "        plot_data['Pobreza']['mean'].plot(ax = axs, marker = '.')\n",
    "        axs.legend(loc = (1, 0))\n",
    "        axs.set_ylim(0, 1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouper in groupers:\n",
    "    info = pd.read_json('./../data/results/result_'+base_str+'_'+'-'.join(grouper)+'.json').drop('timestamp', axis = 1).drop_duplicates()\n",
    "    \n",
    "    info['Q'] = pd.to_datetime(info['Q'])\n",
    "    \n",
    "    # Mean pobreza, indigencia\n",
    "    \n",
    "    info_ = info.loc[info.sintetico == 'mean'].set_index(['observable'] + grouper)['valor']\n",
    "#     plot_data = info_.unstack([1]).T\n",
    "    \n",
    "    ## Columnas que no son ni el trimestre ni el valor. Identifican las series de tiempo.\n",
    "    time_series_cols = list(col for col in info.columns if col not in ['Q', 'valor'])\n",
    "    plot_data = info.groupby([pd.Grouper(freq = '6M', key='Q')] + time_series_cols).agg({'valor': 'mean'}).unstack(0)['valor'].T\n",
    "    \n",
    "    if len(plot_data) < 50:\n",
    "        fig, axs = plt.subplots(1, figsize = (6, 3.5))\n",
    "#         plot_data.plot(ax = axs, marker = '.')\n",
    "        plot_data['Pobreza']['mean'].plot(ax = axs, marker = '.')\n",
    "        axs.legend(loc = (1, 0))\n",
    "        axs.set_ylim(0, 1)\n",
    "        plt.show()\n",
    "        \n",
    "#     # Median salario\n",
    "\n",
    "#     info_ = info.loc[info.sintetico == 'median'].set_index(['observable'] + grouper)['valor']\n",
    "\n",
    "#     plot_data = info_.unstack([1]).T\n",
    "    \n",
    "#     if info_.unstack([1]).T.shape[1] < 50:\n",
    "#         fig, axs = plt.subplots(1, figsize = (6, 3.5))\n",
    "#         plot_data.plot(ax = axs, legend = False, marker = '.')\n",
    "#         axs.legend(loc = (1, 0))\n",
    "# #         axs.set_ylim(0, 1)\n",
    "#         axs.set_ylim(0, 8000)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Se carga la informacion de condiciones de pobreza de cada cuatrimestre y computa el valor sintetico (ej. pobreza por provincia/aglomerado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se ahorra algo de tiempo eligiendo las columnas a importar..\n",
    "\n",
    "# %%timeit\n",
    "# pd.read_csv(f)\n",
    "\n",
    "# 2.09 s ± 49.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "# %%timeit\n",
    "# pd.read_csv(f, usecols = grouper + ['Pobreza', 'Indigencia', 'P03', 'P47T_persona', 'HOGAR_REF_ID', 'P47T_hogar', 'CBA', 'CBT', 'gap'])\n",
    "\n",
    "# 1.31 s ± 11.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "#  pero parece que conviene usar el siguiente loop donde cada info trimestral que se carga se agrupa, y luego se juntan los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
